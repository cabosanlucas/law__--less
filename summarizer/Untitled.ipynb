{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import metapy\n",
    "import math\n",
    "idx = metapy.index.make_inverted_index('config.toml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1179\n",
      "5824.0\n",
      "5824\n"
     ]
    }
   ],
   "source": [
    "# Examine number of documents\n",
    "print idx.num_docs()\n",
    "# Number of unique terms in the dataset\n",
    "print idx.unique_terms()\n",
    "# The average document length\n",
    "print idx.avg_doc_length()\n",
    "# The total number of terms\n",
    "print idx.total_corpus_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0414943982531284"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the query object and initialize a ranker\n",
    "query = metapy.index.Document()\n",
    "ranker = metapy.index.JelinekMercer(.0)\n",
    "# To do an IR evaluation, we need to use the queries file and relevance judgements.\n",
    "ev = metapy.index.IREval('config.toml')\n",
    "# We will loop over the queries file and add each result to the IREval object ev.\n",
    "num_results = 10\n",
    "with open('cranfield-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(idx, query, num_results)                            \n",
    "        avg_p = ev.avg_p(results, query_num, num_results)\n",
    "        #print(\"Query {} average precision: {}\".format(query_num + 1, avg_p))\n",
    "ev.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PL2Ranker(metapy.index.RankingFunction):                                            \n",
    "    \"\"\" Create a new ranking function in Python that can be used in MeTA. \"\"\"                                                                          \n",
    "    def __init__(self, some_param=1.0):                                             \n",
    "        self.param = some_param\n",
    "        # You *must* call the base class constructor here!\n",
    "        super(PL2Ranker, self).__init__()                                        \n",
    "\n",
    "    def score_one(self, sd):\n",
    "        \"\"\" You need to override this function to return a score for a single term. For fields available in the score_data sd object, @see https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html \"\"\"\n",
    "        tfn = sd.doc_term_count + math.log(1 + self.param * (sd.avg_dl / sd.doc_size))\n",
    "        lam = 1.0 * sd.num_docs / sd.corpus_term_count\n",
    "        \n",
    "        if tfn <= 0 or lam < 1:\n",
    "            return 0\n",
    "        \n",
    "        term1 = tfn * math.log(tfn * lam, 2.0)\n",
    "        term2 = math.log(math.e, 2.0 ) * (1.0 / lam - tfn)\n",
    "        term3 = 0.5 * math.log(2.0 * math.pi * tfn, 2.0)\n",
    "        n = term1 + term2 + term3\n",
    "        return sd.query_term_weight * n / (tfn + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03436672545561434"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the query object and initialize a ranker\n",
    "query = metapy.index.Document()\n",
    "ranker = PL2Ranker(.0)\n",
    "# To do an IR evaluation, we need to use the queries file and relevance judgements.\n",
    "ev = metapy.index.IREval('config.toml')\n",
    "# We will loop over the queries file and add each result to the IREval object ev.\n",
    "num_results = 10\n",
    "with open('cranfield-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(idx, query, num_results)                            \n",
    "        avg_p = ev.avg_p(results, query_num, num_results)\n",
    "        #print(\"Query {} average precision: {}\".format(query_num + 1, avg_p))\n",
    "ev.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
